{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9.11_neural-style_content1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsPZdZzgQx44"
      },
      "source": [
        "## 9.11.8 循环"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8m5F3BYQ5BM"
      },
      "source": [
        "!pip install icecream\n",
        "%matplotlib inline\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from icecream import ic\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\") \n",
        "# import d2lzh_pytorch as d2l\n",
        "from matplotlib import pyplot as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 均已测试\n",
        "\n",
        "print(device, torch.__version__)\n",
        "content_img = Image.open('/content/content_image1.png').convert('RGB')\n",
        "# content_img = Image.open('/content/content_image1.png')\n",
        "plt.imshow(content_img)\n",
        "style_img = Image.open('/content/style_image1.png').convert('RGB')\n",
        "# style_img = Image.open('/content/style_image1.png')\n",
        "plt.imshow(style_img)\n",
        "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
        "rgb_std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "def preprocess(PIL_img, image_shape):\n",
        "    process = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(image_shape),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
        "    \n",
        "    img_process = process(PIL_img)\n",
        "    ic(img_process.shape)\n",
        "    ic(img_process.unsqueeze(dim = 0).shape)\n",
        "\n",
        "    return img_process.unsqueeze(dim = 0) # (batch_size, 3, H, W)\n",
        "\n",
        "def postprocess(img_tensor):\n",
        "    inv_normalize = torchvision.transforms.Normalize(\n",
        "        mean= -rgb_mean / rgb_std,\n",
        "        std= 1/rgb_std)\n",
        "    ic(img_tensor[0].shape)\n",
        "    to_PIL_image = torchvision.transforms.ToPILImage()\n",
        "    return to_PIL_image(inv_normalize(img_tensor[0].cpu()).clamp(0, 1))\n",
        "!echo $TORCH_HOME # 将会把预训练好的模型下载到此处(没有输出的话默认是.cache/torch)\n",
        "pretrained_net = torchvision.models.vgg19(pretrained=True, progress=True)\n",
        "pretrained_net\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV0G3s_kQ56W"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8xKamZB0rW9"
      },
      "source": [
        "#选的内容图像在VGG中第几层，一个层\n",
        "# loop = [0, 1]\n",
        "loop = [0, 1, 5, 7, 10, 12, 14, 16, 19, 21, 23, 25, 28, 30, 32, 34]\n",
        "len(loop)\n",
        "start = time()\n",
        "\n",
        "#plt画布大小\n",
        "# plt.figure(figsize=(300*5, 450*5))\n",
        "# plt.figure(figsize=(50, 50)) # in inches\n",
        "\n",
        "for num, loop_i in enumerate(loop):\n",
        "  # plt.subplot(5,5,num+1)\n",
        "  style_layers, content_layers = [0, 5, 10, 19, 28], [loop_i]\n",
        "\n",
        "  #不加入不会用到的层\n",
        "  net_list = []\n",
        "  for i in range(max(content_layers + style_layers) + 1):\n",
        "      net_list.append(pretrained_net.features[i])\n",
        "  # 命名为net\n",
        "  net = torch.nn.Sequential(*net_list)\n",
        "\n",
        "  def extract_features(X, content_layers, style_layers):\n",
        "      contents = []\n",
        "      styles = []\n",
        "      for i in range(len(net)):\n",
        "          #每层的特征\n",
        "          X = net[i](X)\n",
        "          if i in style_layers: \n",
        "              #如果在style层列表里\n",
        "              styles.append(X)\n",
        "          if i in content_layers:\n",
        "              #如果在content层列表里\n",
        "              contents.append(X)\n",
        "      return contents, styles\n",
        "\n",
        "  def get_contents(image_shape, device):\n",
        "      content_X = preprocess(content_img, image_shape).to(device)\n",
        "      contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
        "      return content_X, contents_Y\n",
        "\n",
        "  def get_styles(image_shape, device):\n",
        "      style_X = preprocess(style_img, image_shape).to(device)\n",
        "      _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
        "      return style_X, styles_Y\n",
        "\n",
        "  \"\"\"## 9.11.5 定义损失函数\n",
        "  ### 9.11.5.1 内容损失\n",
        "  \"\"\"\n",
        "\n",
        "  def content_loss(Y_hat, Y):\n",
        "      return F.mse_loss(Y_hat, Y)\n",
        "\n",
        "  \"\"\"### 9.11.5.2 样式损失\"\"\"\n",
        "\n",
        "  def gram(X):\n",
        "      num_channels, n = X.shape[1], X.shape[2] * X.shape[3]\n",
        "      X = X.view(num_channels, n)\n",
        "      return torch.matmul(X, X.t()) / (num_channels * n)\n",
        "\n",
        "  def style_loss(Y_hat, gram_Y):\n",
        "      return F.mse_loss(gram(Y_hat), gram_Y)\n",
        "\n",
        "  \"\"\"### 9.11.5.3 总变差损失\"\"\"\n",
        "\n",
        "  def tv_loss(Y_hat):\n",
        "      return 0.5 * (F.l1_loss(Y_hat[:, :, 1:, :], Y_hat[:, :, :-1, :]) + \n",
        "                    F.l1_loss(Y_hat[:, :, :, 1:], Y_hat[:, :, :, :-1]))\n",
        "\n",
        "  \"\"\"### 9.11.5.4 损失函数\"\"\"\n",
        "\n",
        "  content_weight, style_weight, tv_weight = 1, 1e3, 10\n",
        "\n",
        "  def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n",
        "      # 分别计算内容损失、样式损失和总变差损失\n",
        "      contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n",
        "          contents_Y_hat, contents_Y)]\n",
        "      styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n",
        "          styles_Y_hat, styles_Y_gram)]\n",
        "      tv_l = tv_loss(X) * tv_weight\n",
        "      # 对所有损失求和\n",
        "      l = sum(styles_l) + sum(contents_l) + tv_l\n",
        "      return contents_l, styles_l, tv_l, l\n",
        "\n",
        "  \"\"\"## 9.11.6 创建和初始化合成图像\"\"\"\n",
        "\n",
        "  class GeneratedImage(torch.nn.Module):\n",
        "      def __init__(self, img_shape):\n",
        "          super(GeneratedImage, self).__init__()\n",
        "          self.weight = torch.nn.Parameter(torch.rand(*img_shape))\n",
        "\n",
        "      def forward(self):\n",
        "          return self.weight\n",
        "\n",
        "  def get_inits(X, device, lr, styles_Y):\n",
        "      gen_img = GeneratedImage(X.shape).to(device)\n",
        "      gen_img.weight.data = X.data\n",
        "      optimizer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n",
        "      styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
        "      return gen_img(), styles_Y_gram, optimizer\n",
        "\n",
        "  \"\"\"## 9.11.7 训练\"\"\"\n",
        "\n",
        "  # Commented out IPython magic to ensure Python compatibility.\n",
        "  def train(X, contents_Y, styles_Y, device, lr, max_epochs, lr_decay_epoch):\n",
        "      print(\"training on \", device)\n",
        "      X, styles_Y_gram, optimizer = get_inits(X, device, lr, styles_Y)\n",
        "      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_decay_epoch, gamma=0.1)\n",
        "      for i in range(max_epochs):\n",
        "          start = time.time()\n",
        "          \n",
        "          contents_Y_hat, styles_Y_hat = extract_features(\n",
        "                  X, content_layers, style_layers)\n",
        "          contents_l, styles_l, tv_l, l = compute_loss(\n",
        "                  X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          l.backward(retain_graph = True)\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "          \n",
        "          if i % 50 == 0 and i != 0:\n",
        "              print('epoch %3d, content loss %.2f, style loss %.2f, '\n",
        "                    'TV loss %.2f, %.2f sec'\n",
        "                     % (i, sum(contents_l).item(), sum(styles_l).item(), tv_l.item(),\n",
        "                      time.time() - start))\n",
        "      return X.detach()\n",
        "\n",
        "  image_shape = (300, 450)\n",
        "  _, content_Y = get_contents(image_shape, device)\n",
        "  _, style_Y = get_styles(image_shape, device)\n",
        "  X = preprocess(postprocess(output), image_shape).to(device)\n",
        "  big_output = train(X, content_Y, style_Y, device, 0.01, 500, 200)\n",
        "\n",
        "  # d2l.set_figsize((7, 5))\n",
        "  # d2l.plt.imshow(postprocess(big_output));\n",
        "  \n",
        "  plt.imshow(postprocess(big_output))\n",
        "  # plt.xlabel()\n",
        "  plt.savefig(f'content image layer {loop_i}.png')\n",
        "\n",
        "sec = time()-start\n",
        "ic(f'{sec} s = {sec//60} m')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBEEk6ni0yUs"
      },
      "source": [
        "# plt.figure(figsize=(50, 50))\n",
        "# plt.imshow(postprocess(big_output))\n",
        "\n",
        "# # plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQxgqm9n0y5d"
      },
      "source": [
        "# !cp 'loop.png' /content/drive/MyDrive/QNN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKiyjRZTQw8F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}